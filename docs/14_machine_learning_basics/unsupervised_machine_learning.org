<<12032af8-547f-486b-baab-d9f3f46cf957>>
* Unsupervised machine learning
  :PROPERTIES:
  :CUSTOM_ID: unsupervised-machine-learning
  :END:
Unsupervised machine learning is a technique for configuring (learning)
parameters of a computational model based on no annotation but
additional information such as number of categories to differentiate.
Many algorithms in this category perform data /clustering/.

See also

- [[https://en.wikipedia.org/wiki/Unsupervised_learning][Unsupervised
  learning (Wikipedia)]]
- [[https://scikit-learn.org/stable/modules/clustering.html][scikit-learn's
  clustering algorithms]]

<<825d6705-cf79-4b52-acc6-da93d6e2d96c>>
#+begin_src python
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.cluster import KMeans

# local import; this library is located in the same folder as the notebook
from data_generator import generate_biomodal_2d_data
#+end_src

<<45a2d7ac-3069-42f0-a89f-97bda7adbc08>>
Our starting point for demonstrating supervised machine learning is a a
pair of measurements in a bimodal distribution. In the following data
set objects with a larger area are typically also more elongated.

<<8e3707f7-77be-4a26-811d-3e6bdd47018e>>
#+begin_src python
data = generate_biomodal_2d_data()

plt.scatter(data[:, 0], data[:, 1], c='grey')
#+end_src

#+begin_example
<matplotlib.collections.PathCollection at 0x7fd53c2d7c70>
#+end_example

[[file:f0c50a65a132668537ca93447a617f28bc16d7cb.png]]

<<f43dd438-0f97-4fd3-8488-472afacbcb3e>>
In case of unsupervised machine learning algorithms, we need to provide
additional information to the algorithm so that it can separate
(/cluster/) the data points into regions meaningfully. What information
we provide depends on the algorithm and on the distribution of the data.
Typically, we select the algorithm depending on the data. In the example
above, we can clearly see two clusters, it is a
[[https://en.wikipedia.org/wiki/Multimodal_distribution][bimodal
distribution]]. In that case, we can specify the number of classes to
differentiate:

<<202b9b41-1f52-4e00-b292-496f139aeb4a>>
#+begin_src python
number_of_classes = 2
#+end_src

<<22c17985-c857-4170-84ce-66e96f8e4971>>
** Initializing k-means clustering
   :PROPERTIES:
   :CUSTOM_ID: initializing-k-means-clustering
   :END:
[[https://en.wikipedia.org/wiki/K-means_clustering][k-mean clustering]]
is an algorithm that clusters data points into =k= clusters so that all
data points are assigned to the closest center of the clusters.

Clustering algorithms in scikit-learn typically have a =fit()= function
that consumes a =data= set as given above.

<<ec60e60e-bc9e-4e46-84a3-90366cf45c99>>
#+begin_src python
classifier = KMeans(n_clusters=number_of_classes)
classifier.fit(data)
#+end_src

#+begin_example
KMeans(n_clusters=2)
#+end_example

<<178cc4fa-c62b-415e-9143-89772e44ab83>>
** Prediction
   :PROPERTIES:
   :CUSTOM_ID: prediction
   :END:
Afte the model is trained (or /fitted/), we can apply it to our data to
retrieve a prediction to which cluster the data points belong. Indexing
of clusters starts at 0. Thus, if we asked to differentiate two
clusters, the cluster indices are 0 and 1:

<<5a24c872-f9d4-4349-8b14-541a1de780a8>>
#+begin_src python
prediction = classifier.predict(data)
prediction
#+end_src

#+begin_example
array([0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
       0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
       1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
       1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
       1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
       0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1,
       1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
       0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
       0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
       0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
       0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
       0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
       0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
       1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1], dtype=int32)
#+end_example

<<3bd6d7d5-dac1-4fc7-b529-9c9d46012e9d>>
We can then visualize all the predicted classes in colors.

<<f35dce87-89e0-4fec-ba16-020188f6bdb3>>
#+begin_src python
colors = ['orange', 'blue']
predicted_colors = [colors[i] for i in prediction]

plt.scatter(data[:, 0], data[:, 1], c=predicted_colors)
#+end_src

#+begin_example
<matplotlib.collections.PathCollection at 0x7fd4d8018d30>
#+end_example

[[file:8d6efcc1b5c2797661f0d2902f2c29c5288ad335.png]]

<<7490f220-9ee5-47cb-bad4-0fc192dfdc60>>
** Exercise
   :PROPERTIES:
   :CUSTOM_ID: exercise
   :END:
Train a [[][Gaussian Mixture Model]] and visualize its prediction.

<<e24268a4-db5d-4bbf-9fca-f3c89ef512b8>>
#+begin_src python
from sklearn.mixture import GaussianMixture

classifier = GaussianMixture(n_components=2)
#+end_src

<<dd3dc4cd-6e3a-49ff-8482-4d58812d3e7b>>
#+begin_src python
#+end_src
